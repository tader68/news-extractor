from flask import Flask, render_template, request, jsonify
import requests
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import urllib3
import ssl
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
import os
import google.generativeai as genai

# T·∫Øt c·∫£nh b√°o SSL kh√¥ng an to√†n
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

app = Flask(__name__)

# --- C·∫•u h√¨nh Gemini API ---
try:
    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        print("‚úÖ Gemini API configured successfully.")
    else:
        model = None
        print("‚ö†Ô∏è GEMINI_API_KEY not set. Gemini fallback will be disabled.")
except Exception as e:
    model = None
    print(f"‚ùå Error configuring Gemini API: {e}")


def parse_html_content(html_content, url):
    """
    T√°ch logic tr√≠ch xu·∫•t ti√™u ƒë·ªÅ v√† n·ªôi dung t·ª´ HTML.
    """
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        
        # --- Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ ---
        title = None
        
        # X·ª≠ l√Ω ri√™ng cho tienphong.vn
        if 'tienphong.vn' in url:
            title_element = soup.find('h1', class_='detail-title')
            if title_element:
                title = title_element.get_text(strip=True)
        
        # Fallback cho c√°c trang kh√°c
        if not title:
            title_selectors = [
                'h1.detail-title',
                'h1.article-title',
                'h1.title',
                'h1.post-title',
                'h1[class*="title"]',
                'h1'
            ]
            
            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    title = title_element.get_text(strip=True)
                    break
        
        # --- Tr√≠ch xu·∫•t n·ªôi dung ---
        content = ""
        
        # X·ª≠ l√Ω ri√™ng cho vietnamnet.vn
        if 'vietnamnet.vn' in url:
            sapo_element = soup.find('h2', class_='content-detail-sapo')
            if sapo_element:
                sapo_text = sapo_element.get_text(strip=True)
                if not sapo_text.endswith('...'):
                    content = sapo_text
        
        # X·ª≠ l√Ω ri√™ng cho baotintuc.vn
        elif 'baotintuc.vn' in url:
            sapo_element = soup.find('h2', class_='sapo')
            if sapo_element:
                content = sapo_element.get_text(strip=True)
        
        # X·ª≠ l√Ω ri√™ng cho tienphong.vn
        elif 'tienphong.vn' in url:
            sapo_element = soup.find('h2', class_='sapo')
            if sapo_element:
                content = sapo_element.get_text(strip=True)
        
        # N·∫øu ch∆∞a c√≥ content ho·∫∑c ch∆∞a ƒë·ªß 80 t·ª´, t√¨m th√™m
        if not content or len(content.split()) < 80:
            content_selectors = [
                'div.detail-content p',
                'div.article-content p',
                'div.content p',
                'div.post-content p',
                'div[class*="content"] p',
                'p'
            ]
            
            paragraphs = []
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    for p in elements:
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:
                            paragraphs.append(text)
                    break
            
            # K·∫øt h·ª£p v·ªõi content hi·ªán t·∫°i n·∫øu c√≥
            if content:
                all_content = [content] + paragraphs
            else:
                all_content = paragraphs
            
            # K·∫øt h·ª£p cho ƒë·∫øn khi ƒë·∫°t 80 t·ª´
            combined_content = ""
            for paragraph in all_content:
                if len((combined_content + " " + paragraph).split()) <= 200:  # Gi·ªõi h·∫°n t·ªëi ƒëa
                    combined_content += " " + paragraph if combined_content else paragraph
                    if len(combined_content.split()) >= 80:
                        break
            
            content = combined_content.strip()
        
        # L√†m s·∫°ch content
        if content:
            content = content.replace('...', '').strip()
        
        return title, content
        
    except Exception as e:
        print(f"‚ùå Error parsing HTML: {e}")
        return None, ""


def extract_with_selenium(url):
    """
    S·ª≠ d·ª•ng Selenium ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung (Phi√™n b·∫£n ·ªïn ƒë·ªãnh cho Cloud).
    """
    print(f"üöÄ Attempting Selenium extraction for: {url}")
    
    # --- C·∫•u h√¨nh Chrome Options T·ªëi ∆∞u cho Cloud ---
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    # QUAN TR·ªåNG: Kh√¥ng ch·ªâ ƒë·ªãnh --user-data-dir, ƒë·ªÉ Selenium t·ª± qu·∫£n l√Ω profile t·∫°m.
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.implicitly_wait(15) # TƒÉng th·ªùi gian ch·ªù ng·∫ßm
        
        print(f"‚úÖ Driver created. Navigating to URL...")
        driver.get(url)
        time.sleep(3)
        
        html_content = driver.page_source
        if not html_content or len(html_content) < 500: # Ki·ªÉm tra n·ªôi dung c√≥ h·ª£p l·ªá kh√¥ng
             raise ValueError("Page source is too short or empty.")

        # S·ª≠ d·ª•ng parse_html_content function ƒë·ªÉ tr√≠ch xu·∫•t
        title, content = parse_html_content(html_content, url)
        
        if title and content:
            print("‚úÖ Selenium extraction successful!")
            return {"title": title, "content": content, "success": True}
        
        print("‚ö†Ô∏è Selenium did not find enough content.")
        return {"title": title or "Kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ", "content": "Kh√¥ng t√¨m th·∫•y n·ªôi dung", "success": False}

    except Exception as e:
        print(f"‚ùå Selenium Error: {str(e)}")
        return {"title": "L·ªói Selenium", "content": f"Kh√¥ng th·ªÉ s·ª≠ d·ª•ng Selenium: {str(e)}", "success": False}
    finally:
        if driver:
            driver.quit()
            print("‚úÖ Chrome driver closed.")


def extract_with_gemini(url):
    """
    S·ª≠ d·ª•ng Gemini API l√†m ph∆∞∆°ng √°n d·ª± ph√≤ng cu·ªëi c√πng.
    """
    if not model:
        return {"title": "L·ªói Gemini", "content": "Gemini API ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.", "success": False}
        
    print(f"üîÑ Falling back to Gemini API for: {url}")
    try:
        prompt = f"""
        Truy c·∫≠p URL sau v√† tr√≠ch xu·∫•t th√¥ng tin: {url}
        H√£y th·ª±c hi·ªán c√°c y√™u c·∫ßu sau:
        1.  Tr√≠ch xu·∫•t TI√äU ƒê·ªÄ CH√çNH c·ªßa b√†i b√°o.
        2.  Tr√≠ch xu·∫•t ƒêO·∫†N VƒÇN ƒê·∫¶U TI√äN (sapo ho·∫∑c ƒëo·∫°n m·ªü ƒë·∫ßu) c·ªßa b√†i b√°o. ƒê·∫£m b·∫£o n·ªôi dung ƒë·∫ßy ƒë·ªß, kh√¥ng b·ªã c·∫Øt ng·∫Øn.
        
        Ch·ªâ tr·∫£ l·ªùi v·ªõi ƒë·ªãnh d·∫°ng JSON sau, kh√¥ng th√™m b·∫•t c·ª© gi·∫£i th√≠ch n√†o:
        {{
          "title": "ti√™u ƒë·ªÅ b√†i b√°o ·ªü ƒë√¢y",
          "content": "ƒëo·∫°n vƒÉn ƒë·∫ßu ti√™n ·ªü ƒë√¢y"
        }}
        """
        response = model.generate_content(prompt)
        
        # X·ª≠ l√Ω response ƒë·ªÉ l·∫•y JSON
        import json
        clean_response = response.text.strip().replace('```json', '').replace('```', '')
        data = json.loads(clean_response)
        
        if data.get("title") and data.get("content"):
            print("‚úÖ Gemini extraction successful!")
            data["success"] = True
            return data
        else:
            raise ValueError("Invalid JSON structure from Gemini.")
            
    except Exception as e:
        print(f"‚ùå Gemini API Error: {str(e)}")
        return {"title": "L·ªói Gemini API", "content": f"Kh√¥ng th·ªÉ x·ª≠ l√Ω URL b·∫±ng Gemini: {str(e)}", "success": False}


def extract_title_and_content(url):
    """
    H√†m ch√≠nh ƒëi·ªÅu ph·ªëi vi·ªác tr√≠ch xu·∫•t, th·ª≠ c√°c ph∆∞∆°ng ph√°p kh√°c nhau.
    """
    print(f"\nProcessing URL: {url}")
    
    # --- PH∆Ø∆†NG PH√ÅP 1: D√πng Requests (Nhanh nh·∫•t) ---
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        
        # S·ª≠ d·ª•ng parse_html_content function ƒë·ªÉ tr√≠ch xu·∫•t
        title, content = parse_html_content(response.content.decode('utf-8'), url)
        
        if title and content:
            print("‚úÖ Requests extraction successful!")
            return build_success_response(title, content)

    except requests.RequestException as e:
        print(f"‚ö†Ô∏è Requests failed: {e}. Trying Selenium.")

    # --- PH∆Ø∆†NG PH√ÅP 2: D√πng Selenium (Fallback) ---
    selenium_result = extract_with_selenium(url)
    if selenium_result['success']:
        return build_success_response(selenium_result['title'], selenium_result['content'])

    # --- PH∆Ø∆†NG PH√ÅP 3: D√πng Gemini (Fallback cu·ªëi c√πng) ---
    gemini_result = extract_with_gemini(url)
    if gemini_result['success']:
        return build_success_response(gemini_result['title'], gemini_result['content'])

    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i
    print("‚ùå All extraction methods failed.")
    return build_error_response("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL n√†y sau nhi·ªÅu l·∫ßn th·ª≠.")


def build_success_response(title, content):
    """T·∫°o response th√†nh c√¥ng v√† t√≠nh to√°n s·ªë t·ª´."""
    word_count = {
        'title_words': len(title.split()),
        'content_words': len(content.split()),
        'total_words': len(title.split()) + len(content.split()),
        'meets_minimum': (len(title.split()) + len(content.split())) >= 80
    }
    return {'title': title, 'content': content, 'word_count': word_count, 'success': True}


def build_error_response(message):
    """T·∫°o response l·ªói."""
    return {'title': 'L·ªói', 'content': message, 'word_count': {'total_words': 0, 'meets_minimum': False}, 'success': False}


async def extract_title_and_content_async(session, url):
    """
    Async version v·ªõi h·ªá th·ªëng fallback 3 t·∫ßng: aiohttp -> Selenium -> Gemini API
    """
    print(f"\nüîç Starting async extraction for: {url}")
    
    # --- PH∆Ø∆†NG PH√ÅP 1: D√πng aiohttp (Nhanh nh·∫•t) ---
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        async with session.get(url, ssl=False, timeout=aiohttp.ClientTimeout(total=30), headers=headers) as response:
            html_content = await response.text()
            title, content = parse_html_content(html_content, url)
            
            if title and content:
                print("‚úÖ Aiohttp extraction successful!")
                return build_success_response(title, content)

    except Exception as e:
        print(f"‚ö†Ô∏è Aiohttp failed: {e}. Trying Selenium.")

    # --- PH∆Ø∆†NG PH√ÅP 2: D√πng Selenium (Fallback) ---
    selenium_result = extract_with_selenium(url)
    if selenium_result['success']:
        return build_success_response(selenium_result['title'], selenium_result['content'])

    # --- PH∆Ø∆†NG PH√ÅP 3: D√πng Gemini (Fallback cu·ªëi c√πng) ---
    gemini_result = extract_with_gemini(url)
    if gemini_result['success']:
        return build_success_response(gemini_result['title'], gemini_result['content'])

    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i
    print("‚ùå All extraction methods failed.")
    return build_error_response("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL n√†y sau nhi·ªÅu l·∫ßn th·ª≠.")


async def batch_extract_async(urls):
    """
    Async batch processing v·ªõi song song ho√° ho√†n to√†n
    """
    print(f"üöÄ Starting async batch extraction for {len(urls)} URLs")
    
    async with aiohttp.ClientSession() as session:
        # T·∫°o tasks cho t·∫•t c·∫£ URLs ƒë·ªìng th·ªùi
        tasks = [extract_title_and_content_async(session, url) for url in urls]
        
        # Ch·ªù t·∫•t c·∫£ ho√†n th√†nh ƒë·ªìng th·ªùi
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ v√† exception
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_result = build_error_response(f"L·ªói x·ª≠ l√Ω: {str(result)}")
                error_result['url'] = urls[i]
                processed_results.append(error_result)
            else:
                result['url'] = urls[i]
                processed_results.append(result)
        
        return processed_results


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/health')
def health_check():
    """Health check endpoint for Render"""
    return {'status': 'healthy', 'service': 'news-extractor'}, 200

@app.route('/extract', methods=['POST'])
def extract():
    url = (request.get_json() or {}).get('url') or request.form.get('url', '')
    if not url:
        return jsonify(build_error_response('Vui l√≤ng nh·∫≠p URL h·ª£p l·ªá.'))
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        
    result = extract_title_and_content(url)
    return jsonify(result)


@app.route('/batch_extract', methods=['POST'])
def batch_extract():
    urls_text = request.form.get('urls', '')
    if not urls_text:
        return jsonify({'results': [], 'error': 'Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt URL.'})
    
    urls = [url.strip() for url in urls_text.splitlines() if url.strip()]
    full_urls = [url if url.startswith(('http://', 'https://')) else 'https://' + url for url in urls]
    
    # S·ª≠ d·ª•ng async batch processing
    results = asyncio.run(batch_extract_async(full_urls))
        
    return jsonify({'results': results})


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=False, host='0.0.0.0', port=port)
