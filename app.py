from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup
import urllib3
import ssl
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
import os
import google.generativeai as genai

# T·∫Øt c·∫£nh b√°o SSL kh√¥ng an to√†n
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

app = Flask(__name__)

# --- C·∫•u h√¨nh Gemini API ---
try:
    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        print("‚úÖ Gemini API configured successfully.")
    else:
        model = None
        print("‚ö†Ô∏è GEMINI_API_KEY not set. Gemini fallback will be disabled.")
except Exception as e:
    model = None
    print(f"‚ùå Error configuring Gemini API: {e}")


def extract_with_selenium(url):
    """
    S·ª≠ d·ª•ng Selenium ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung (Phi√™n b·∫£n ·ªïn ƒë·ªãnh cho Cloud).
    """
    print(f"üöÄ Attempting Selenium extraction for: {url}")
    
    # --- C·∫•u h√¨nh Chrome Options T·ªëi ∆∞u cho Cloud ---
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    # QUAN TR·ªåNG: Kh√¥ng ch·ªâ ƒë·ªãnh --user-data-dir, ƒë·ªÉ Selenium t·ª± qu·∫£n l√Ω profile t·∫°m.
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.implicitly_wait(15) # TƒÉng th·ªùi gian ch·ªù ng·∫ßm
        
        print(f"‚úÖ Driver created. Navigating to URL...")
        driver.get(url)
        time.sleep(3)
        
        html_content = driver.page_source
        if not html_content or len(html_content) < 500: # Ki·ªÉm tra n·ªôi dung c√≥ h·ª£p l·ªá kh√¥ng
             raise ValueError("Page source is too short or empty.")

        soup = BeautifulSoup(html_content, 'html.parser')
        
        # (Logic tr√≠ch xu·∫•t ti√™u ƒë·ªÅ v√† n·ªôi dung gi·ªØ nguy√™n)
        title, content = None, None
        title_selectors = ['h1.title', 'h1.detail-title', 'h1', '.entry-title', 'title']
        for selector in title_selectors:
            if title_element := soup.select_one(selector):
                if len(title_element.get_text(strip=True)) > 10:
                    title = title_element.get_text(strip=True)
                    break
        
        # C√°c logic t√¨m content c√≥ th·ªÉ th√™m v√†o ƒë√¢y
        # ∆Øu ti√™n meta description
        if meta_desc := soup.find('meta', attrs={'name': 'description'}):
            if len(meta_desc.get('content', '')) > 50:
                content = meta_desc.get('content', '').strip()

        # N·∫øu kh√¥ng c√≥, t√¨m trong c√°c th·∫ª <p>
        if not content:
            for p in soup.find_all('p'):
                text = p.get_text(strip=True)
                if len(text) > 80 and 'login' not in text.lower() and 'ƒëƒÉng nh·∫≠p' not in text.lower():
                    content = text
                    break
        
        # N·∫øu content < 80 t·ª´, l·∫•y th√™m ƒëo·∫°n vƒÉn k·∫ø ti·∫øp (t·∫•t c·∫£ trang ƒë·ªÅu √°p d·ª•ng)
        if content and len(content.split()) < 80:
            print(f"Selenium: Content has {len(content.split())} words, adding more paragraphs...")
            all_paragraphs = soup.find_all('p')
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                if (len(text) > 30 and 
                    'login' not in text.lower() and 
                    'ƒëƒÉng nh·∫≠p' not in text.lower() and
                    'ngu·ªìn:' not in text.lower() and
                    '·∫£nh:' not in text.lower() and
                    'photo:' not in text.lower() and
                    'image:' not in text.lower() and
                    text not in content):
                    content += " " + text
                    print(f"Selenium: Combined content now has {len(content.split())} words")
                    if len(content.split()) >= 80:
                        print("‚úÖ Selenium: Content combined successfully to meet 80-word minimum.")
                        break
        
        if title and content:
            print("‚úÖ Selenium extraction successful!")
            return {"title": title, "content": content, "success": True}
        
        print("‚ö†Ô∏è Selenium did not find enough content.")
        return {"title": title or "Kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ", "content": "Kh√¥ng t√¨m th·∫•y n·ªôi dung", "success": False}

    except Exception as e:
        print(f"‚ùå Selenium Error: {str(e)}")
        return {"title": "L·ªói Selenium", "content": f"Kh√¥ng th·ªÉ s·ª≠ d·ª•ng Selenium: {str(e)}", "success": False}
    finally:
        if driver:
            driver.quit()
            print("‚úÖ Chrome driver closed.")


def extract_with_gemini(url):
    """
    S·ª≠ d·ª•ng Gemini API l√†m ph∆∞∆°ng √°n d·ª± ph√≤ng cu·ªëi c√πng.
    """
    if not model:
        return {"title": "L·ªói Gemini", "content": "Gemini API ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.", "success": False}
        
    print(f"üîÑ Falling back to Gemini API for: {url}")
    try:
        prompt = f"""
        Truy c·∫≠p URL sau v√† tr√≠ch xu·∫•t th√¥ng tin: {url}
        H√£y th·ª±c hi·ªán c√°c y√™u c·∫ßu sau:
        1.  Tr√≠ch xu·∫•t TI√äU ƒê·ªÄ CH√çNH c·ªßa b√†i b√°o.
        2.  Tr√≠ch xu·∫•t ƒêO·∫†N VƒÇN ƒê·∫¶U TI√äN (sapo ho·∫∑c ƒëo·∫°n m·ªü ƒë·∫ßu) c·ªßa b√†i b√°o. ƒê·∫£m b·∫£o n·ªôi dung ƒë·∫ßy ƒë·ªß, kh√¥ng b·ªã c·∫Øt ng·∫Øn.
        
        Ch·ªâ tr·∫£ l·ªùi v·ªõi ƒë·ªãnh d·∫°ng JSON sau, kh√¥ng th√™m b·∫•t c·ª© gi·∫£i th√≠ch n√†o:
        {{
          "title": "ti√™u ƒë·ªÅ b√†i b√°o ·ªü ƒë√¢y",
          "content": "ƒëo·∫°n vƒÉn ƒë·∫ßu ti√™n ·ªü ƒë√¢y"
        }}
        """
        response = model.generate_content(prompt)
        
        # X·ª≠ l√Ω response ƒë·ªÉ l·∫•y JSON
        import json
        clean_response = response.text.strip().replace('```json', '').replace('```', '')
        data = json.loads(clean_response)
        
        if data.get("title") and data.get("content"):
            print("‚úÖ Gemini extraction successful!")
            data["success"] = True
            return data
        else:
            raise ValueError("Invalid JSON structure from Gemini.")
            
    except Exception as e:
        print(f"‚ùå Gemini API Error: {str(e)}")
        return {"title": "L·ªói Gemini API", "content": f"Kh√¥ng th·ªÉ x·ª≠ l√Ω URL b·∫±ng Gemini: {str(e)}", "success": False}


def extract_title_and_content(url):
    """
    H√†m ch√≠nh ƒëi·ªÅu ph·ªëi vi·ªác tr√≠ch xu·∫•t, th·ª≠ c√°c ph∆∞∆°ng ph√°p kh√°c nhau.
    """
    print(f"\nProcessing URL: {url}")
    
    # --- PH∆Ø∆†NG PH√ÅP 1: D√πng Requests (Nhanh nh·∫•t) ---
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Logic tr√≠ch xu·∫•t t∆∞∆°ng t·ª± Selenium
        title, content = None, None
        
        # X·ª≠ l√Ω ƒë·∫∑c bi·ªát cho c√°c trang c·ª• th·ªÉ
        if 'vietnamnet.vn' in url:
            print("üîç Applying specific logic for vietnamnet.vn")
            if sapo := soup.select_one('h2.content-detail-sapo, [class*="sapo"]'):
                sapo_text = sapo.get_text(strip=True)
                if len(sapo_text) > 30 and not sapo_text.endswith('...'):
                    content = sapo_text
                    print(f"‚úÖ VietnamNet sapo extracted: {len(sapo_text.split())} words")
        elif 'tienphong.vn' in url:
            print("üîç Applying specific logic for tienphong.vn")
            if sapo := soup.select_one('div.sapo p, .article-sapo'):
                sapo_text = sapo.get_text(strip=True)
                if len(sapo_text) > 30:
                    content = sapo_text
                    print(f"‚úÖ TienPhong sapo extracted: {len(sapo_text.split())} words")
        elif 'baotintuc.vn' in url:
            print("üîç Applying specific logic for baotintuc.vn")
            if sapo := soup.select_one('h2.sapo, [class="sapo"]'):
                sapo_text = sapo.get_text(strip=True)
                if len(sapo_text) > 50:
                    content = sapo_text
                    print(f"‚úÖ BaoTinTuc sapo extracted: {len(sapo_text.split())} words")

        # Logic chung
        title_selectors = ['h1.title', 'h1.detail-title', 'h1', '.entry-title', 'title']
        for selector in title_selectors:
            if title_element := soup.select_one(selector):
                if len(title_element.get_text(strip=True)) > 10:
                    title = title_element.get_text(strip=True)
                    break
        
        if not content:
            if meta_desc := soup.find('meta', attrs={'name': 'description'}):
                if len(meta_desc.get('content', '')) > 50:
                    content = meta_desc.get('content', '').strip()

        if not content:
            for p in soup.find_all('p'):
                text = p.get_text(strip=True)
                if len(text) > 80 and 'login' not in text.lower() and 'ƒëƒÉng nh·∫≠p' not in text.lower():
                    content = text
                    break
        
        # N·∫øu content < 80 t·ª´, l·∫•y th√™m ƒëo·∫°n vƒÉn k·∫ø ti·∫øp (t·∫•t c·∫£ trang ƒë·ªÅu √°p d·ª•ng)
        if content and len(content.split()) < 80:
            print(f"Requests: Content has {len(content.split())} words, adding more paragraphs...")
            all_paragraphs = soup.find_all('p')
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                if (len(text) > 30 and 
                    'login' not in text.lower() and 
                    'ƒëƒÉng nh·∫≠p' not in text.lower() and
                    'ngu·ªìn:' not in text.lower() and
                    '·∫£nh:' not in text.lower() and
                    'photo:' not in text.lower() and
                    'image:' not in text.lower() and
                    text not in content):
                    content += " " + text
                    print(f"Requests: Combined content now has {len(content.split())} words")
                    if len(content.split()) >= 80:
                        print("‚úÖ Content combined successfully to meet 80-word minimum.")
                        break
        
        if title and content:
            print("‚úÖ Requests extraction successful!")
            return build_success_response(title, content)

    except requests.RequestException as e:
        print(f"‚ö†Ô∏è Requests failed: {e}. Trying Selenium.")

    # --- PH∆Ø∆†NG PH√ÅP 2: D√πng Selenium (Fallback) ---
    selenium_result = extract_with_selenium(url)
    if selenium_result['success']:
        return build_success_response(selenium_result['title'], selenium_result['content'])

    # --- PH∆Ø∆†NG PH√ÅP 3: D√πng Gemini (Fallback cu·ªëi c√πng) ---
    gemini_result = extract_with_gemini(url)
    if gemini_result['success']:
        return build_success_response(gemini_result['title'], gemini_result['content'])

    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i
    print("‚ùå All extraction methods failed.")
    return build_error_response("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL n√†y sau nhi·ªÅu l·∫ßn th·ª≠.")


def build_success_response(title, content):
    """T·∫°o response th√†nh c√¥ng v√† t√≠nh to√°n s·ªë t·ª´."""
    word_count = {
        'title_words': len(title.split()),
        'content_words': len(content.split()),
        'total_words': len(title.split()) + len(content.split()),
        'meets_minimum': (len(title.split()) + len(content.split())) >= 80
    }
    return {'title': title, 'content': content, 'word_count': word_count, 'success': True}


def build_error_response(message):
    """T·∫°o response l·ªói."""
    return {'title': 'L·ªói', 'content': message, 'word_count': {'total_words': 0, 'meets_minimum': False}, 'success': False}


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/health')
def health_check():
    """Health check endpoint for Render"""
    return {'status': 'healthy', 'service': 'news-extractor'}, 200

@app.route('/extract', methods=['POST'])
def extract():
    url = (request.get_json() or {}).get('url') or request.form.get('url', '')
    if not url:
        return jsonify(build_error_response('Vui l√≤ng nh·∫≠p URL h·ª£p l·ªá.'))
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
        
    result = extract_title_and_content(url)
    return jsonify(result)


@app.route('/batch_extract', methods=['POST'])
def batch_extract():
    urls_text = request.form.get('urls', '')
    if not urls_text:
        return jsonify({'results': [], 'error': 'Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt URL.'})
    
    urls = [url.strip() for url in urls_text.splitlines() if url.strip()]
    results = []
    for url in urls:
        full_url = url if url.startswith(('http://', 'https://')) else 'https://' + url
        result = extract_title_and_content(full_url)
        result['url'] = full_url
        results.append(result)
        
    return jsonify({'results': results})


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=False, host='0.0.0.0', port=port)
